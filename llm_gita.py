# -*- coding: utf-8 -*-
"""LLM Gita.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wBTjmFK2F3tRDAux-uJf8Y8XR7POV5_Y
"""

from google.colab import files
uploaded = files.upload()

!pip install sentence-transformers faiss-cpu

import pandas as pd


df = pd.read_csv("bhagavad_gita.csv")


df_clean = df[['verse_number', 'translation_in_english', 'meaning_in_english']].copy()
df_clean['text'] = df_clean['meaning_in_english'].fillna(df_clean['translation_in_english'])
df_clean = df_clean.dropna(subset=['text'])
df_clean['context'] = "Verse " + df_clean['verse_number'].astype(str) + ":\n" + df_clean['text']
texts = df_clean['context'].tolist()

from sentence_transformers import SentenceTransformer

model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
embeddings = model.encode(texts, show_progress_bar=True)

import faiss
import numpy as np

embeddings_np = np.array(embeddings).astype('float32')
index = faiss.IndexFlatL2(embeddings_np.shape[1])
index.add(embeddings_np)

print("FAISS index built with", index.ntotal, "vectors")

import pickle

faiss.write_index(index, "gita_index.faiss")
with open("gita_texts.pkl", "wb") as f:
    pickle.dump(texts, f)

import faiss
import pickle


def search_verses(query, top_k=3):
    query_embedding = model.encode([query]).astype('float32')
    distances, indices = index.search(query_embedding, top_k)
    results = [texts[i] for i in indices[0]]
    return results

query = "What does Krishna say about desire?"
top_results = search_verses(query)

print("üîç Retrieved Verses:\n")
for i, res in enumerate(top_results, 1):
    print(f"[{i}] {res}\n")

!pip install -U langchain langchain-huggingface huggingface_hub

from langchain_huggingface import HuggingFaceEndpoint
from langchain_core.messages import HumanMessage

from langchain.llms import HuggingFaceEndpoint

from huggingface_hub import InferenceClient

HUGGINGFACE_API_TOKEN = "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"

client = InferenceClient(
    model="meta-llama/Meta-Llama-3-8B-Instruct",
    token=HUGGINGFACE_API_TOKEN
)


context = "\n\n".join(top_results)


prompt = f"""
You are a spiritual assistant answering based on the Bhagavad Gita.


Context:
{context}

Question: What should I do when I am angry?

Answer:
"""


response = client.chat_completion(
    messages=[{"role": "user", "content": prompt}],
    max_tokens=400,
    temperature=0.7,
)

print(response.choices[0].message["content"])